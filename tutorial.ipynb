{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Fine-tuning Rcnn Object Detection Pipeline\n",
    "\n",
    "UPDATE COLAB BADGE LINK\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/unionai-oss/object-detection-computer-vision-pipeline/blob/main/tutorial.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "GitHub repository: https://github.com/unionai-oss/object-detection-computer-vision-pipeline\n",
    "\n",
    "This notebook is a pipeline for fine-tuning a fast rcnn model on a custom dataset with PyTorch\n",
    "\n",
    "## Project Setup:\n",
    "\n",
    "Sign up for Union while libraries are installing below:\n",
    "\n",
    "- Union sign up: https://signup.union.ai/\n",
    "- Union Dashboard: https://serverless.union.ai/\n",
    "\n",
    "Install python libraries by running the code cell below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install required libraries\n",
    "!pip install python-dotenv union==0.1.64 torch torchvision matplotlib pycocotools datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auth Union account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!union create login --auth device-flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Simple Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile simple_wf.py\n",
    "\n",
    "# Import libraries and modules\n",
    "# task\n",
    "from flytekit import task, workflow\n",
    "\n",
    "@task\n",
    "def hello_world(name: str) -> str:\n",
    "    return f\"Hello {name}\"\n",
    "\n",
    "# workflow\n",
    "@workflow\n",
    "def main(name: str) -> str:\n",
    "    return hello_world(name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run locally\n",
    "!union run simple_wf.py main --name Flyte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run on Union\n",
    "!union run --remote simple_wf.py main --name Flyte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!union run simple_wf.py main --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!union run --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine tune Fast RCNN Model for Object Detection\n",
    "- Download the dataset\n",
    "- Download the model\n",
    "- Train the model\n",
    "- Evaluate the model\n",
    "\n",
    "Use the model on new images or video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile workflow.py\n",
    "\n",
    "\"\"\"\n",
    "Object Detection Workflow with Flyte and PyTorch using the Faster R-CNN model\n",
    "\n",
    "note: This Flyte workflow can be broken out into modular tasks for better organization and reusability!\n",
    "\"\"\"\n",
    "# %%\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from flytekit import task, workflow, ImageSpec, Resources, current_context, Deck\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CocoDetection\n",
    "from torchvision.models.detection.faster_rcnn import \\\n",
    "    FasterRCNN_ResNet50_FPN_Weights\n",
    "from torchvision.ops import box_iou\n",
    "from torchvision.transforms import transforms as T\n",
    "from flytekit.types.directory import FlyteDirectory\n",
    "from flytekit.types.file import FlyteFile\n",
    "import base64\n",
    "from textwrap import dedent\n",
    "\n",
    "\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "import requests\n",
    "\n",
    "\n",
    "image = ImageSpec(\n",
    "    packages=[\n",
    "        \"union==0.1.64\",\n",
    "        \"torch\",\n",
    "        \"torchvision\",\n",
    "        \"matplotlib\",\n",
    "        \"pycocotools\",\n",
    "        \"datasets\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Check and set the available device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "hyperparams = {\n",
    "    \"batch_size\": 2,\n",
    "    \"num_workers\": 4,\n",
    "    \"lr\": 0.005,\n",
    "    \"momentum\": 0.9,\n",
    "    \"weight_decay\": 0.0005,\n",
    "    \"step_size\": 3,\n",
    "    \"gamma\": 0.1,\n",
    "    \"num_epochs\": 1,\n",
    "}\n",
    "\n",
    "\n",
    "# %% ------------------------------\n",
    "# Data loading helper functions\n",
    "# --------------------------------\n",
    "\n",
    "# Convert images to base64 and embed in HTML\n",
    "def image_to_base64(image_path):\n",
    "    with open(image_path, \"rb\") as img_file:\n",
    "        return base64.b64encode(img_file.read()).decode(\"utf-8\")\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "def dataset_dataloader(\n",
    "                    root: str,\n",
    "                    annFile: str,\n",
    "                    batch_size=2,\n",
    "                    shuffle=True,\n",
    "                    num_workers=0,\n",
    "                ) -> DataLoader:\n",
    "    # Define the transformations for the images\n",
    "    transform = T.Compose([T.ToTensor()])\n",
    "\n",
    "    annFile_path = os.path.join(str(root), annFile)\n",
    "    print(f\"Annotation file path: {annFile_path}\")  # Debugging the annotation file path\n",
    "\n",
    "\n",
    "    # Load the dataset\n",
    "    dataset = CocoDetection(root=root, annFile=annFile_path, transform=transform)\n",
    "    data_loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=num_workers,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "\n",
    "    return data_loader\n",
    "\n",
    "\n",
    "# %% ------------------------------\n",
    "# Download dataset - task\n",
    "# --------------------------------\n",
    "@task(container_image=image,\n",
    "      enable_deck=True,\n",
    "      cache=True,\n",
    "      cache_version=\"1.2\",\n",
    "      requests=Resources(cpu=\"2\", mem=\"2Gi\")) \n",
    "\n",
    "def download_hf_dataset(repo_id: str = 'sagecodes/union_swag_coco',\n",
    "                        local_dir: str = \"dataset\",\n",
    "                        sub_folder: str = \"swag\") -> FlyteDirectory:\n",
    "    \n",
    "    from huggingface_hub import snapshot_download\n",
    "\n",
    "    if local_dir:\n",
    "        dataset_dir = os.path.join(local_dir)\n",
    "        os.makedirs(dataset_dir, exist_ok=True)\n",
    "\n",
    "    # Download the dataset repository\n",
    "    repo_path = snapshot_download(repo_id=repo_id, \n",
    "                                  repo_type=\"dataset\",\n",
    "                                  local_dir=local_dir)\n",
    "    if sub_folder:\n",
    "        repo_path = os.path.join(repo_path, sub_folder)\n",
    "        # use sub_folder to return a specific folder from the dataset\n",
    "\n",
    "    print(f\"Dataset downloaded to {repo_path}\")\n",
    "\n",
    "    print(f\"Files in dataset directory: {os.listdir(repo_path)}\")\n",
    "\n",
    "    return FlyteDirectory(repo_path)\n",
    "\n",
    "# %% ------------------------------\n",
    "# visualize data - task\n",
    "# --------------------------------\n",
    "@task(container_image=image,\n",
    "    requests=Resources(cpu=\"2\", mem=\"2Gi\"))\n",
    "def visualize_data():\n",
    "    data_loader = dataset_dataloader(\"data/swag\", \"data/swag/train.json\")\n",
    "\n",
    "    # Function to display an image with its bounding boxes\n",
    "    def show_image_with_boxes(image, annotations):\n",
    "        fig, ax = plt.subplots(1)\n",
    "        ax.imshow(image.permute(1, 2, 0))\n",
    "\n",
    "        for annotation in annotations:\n",
    "            bbox = annotation[\"bbox\"]\n",
    "            rect = patches.Rectangle(\n",
    "                (bbox[0], bbox[1]),\n",
    "                bbox[2],\n",
    "                bbox[3],\n",
    "                linewidth=1,\n",
    "                edgecolor=\"r\",\n",
    "                facecolor=\"none\",\n",
    "            )\n",
    "            ax.add_patch(rect)\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    # Visualize a batch of images\n",
    "    for images, targets in data_loader:\n",
    "        for i, image in enumerate(images):\n",
    "            show_image_with_boxes(image, targets[i])\n",
    "\n",
    "# %% ------------------------------\n",
    "# donwload model - task\n",
    "# --------------------------------\n",
    "@task(container_image=image,\n",
    "    cache=True,\n",
    "    cache_version=\"1.1\",\n",
    "    requests=Resources(cpu=\"2\", mem=\"2Gi\"))\n",
    "def download_model() -> torch.nn.Module:\n",
    "    # Load a pre-trained model\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(\n",
    "        weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1, weights_only=True\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "# %% ------------------------------\n",
    "# train model - task\n",
    "# --------------------------------\n",
    "@task(container_image=image,\n",
    "    requests=Resources(cpu=\"2\", mem=\"8Gi\", gpu=\"1\"))\n",
    "def train_model(model: torch.nn.Module, dataset_dir: FlyteDirectory, num_epochs :int=3) -> torch.nn.Module:\n",
    "\n",
    "    # TODO: make from dict\n",
    "    num_classes = 3  # number of classes + background (TODO: add one for the background class automatically)\n",
    "    num_epochs = num_epochs\n",
    "    best_mean_iou = 0\n",
    "    model_dir = \"models\"\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    dataset_dir.download()\n",
    "\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "    local_dataset_dir = dataset_dir.path  # Use the local path for FlyteDirectory\n",
    "\n",
    "    data_loader = dataset_dataloader(root=local_dataset_dir, annFile=\"train.json\")\n",
    "    test_data_loader = dataset_dataloader(root=local_dataset_dir, annFile=\"train.json\")\n",
    "\n",
    "    # Modify the model to add a new classification head based on the number of classes\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = (\n",
    "        torchvision.models.detection.faster_rcnn.FastRCNNPredictor(\n",
    "            in_features, num_classes\n",
    "        )\n",
    "    )\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    # Define optimizer and learning rate\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "    # Learning rate scheduler\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "\n",
    "    # Function to filter out and correct invalid boxes\n",
    "    def filter_and_correct_boxes(targets):\n",
    "        filtered_targets = []\n",
    "        for target in targets:\n",
    "            boxes = target[\"boxes\"]\n",
    "            labels = target[\"labels\"]\n",
    "            valid_indices = []\n",
    "            for i, box in enumerate(boxes):\n",
    "                if box[2] > box[0] and box[3] > box[1]:\n",
    "                    valid_indices.append(i)\n",
    "                else:\n",
    "                    print(f\"Invalid box found and removed: {box}\")\n",
    "            filtered_boxes = boxes[valid_indices]\n",
    "            filtered_labels = labels[valid_indices]\n",
    "            filtered_targets.append(\n",
    "                {\"boxes\": filtered_boxes, \"labels\": filtered_labels}\n",
    "            )\n",
    "        return filtered_targets\n",
    "\n",
    "    # Function to evaluate the model\n",
    "    def evaluate_model(model, data_loader):\n",
    "        model.eval()\n",
    "        iou_list, loss_list = [], []\n",
    "        correct_predictions, total_predictions = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for images, targets in data_loader:\n",
    "                images = [image.to(device) for image in images]\n",
    "                targets = [\n",
    "                    {\n",
    "                        \"boxes\": torch.tensor(\n",
    "                            [obj[\"bbox\"] for obj in t], dtype=torch.float32\n",
    "                        ).to(device),\n",
    "                        \"labels\": torch.tensor(\n",
    "                            [obj[\"category_id\"] for obj in t], dtype=torch.int64\n",
    "                        ).to(device),\n",
    "                    }\n",
    "                    for t in targets\n",
    "                ]\n",
    "                for target in targets:\n",
    "                    boxes = target[\"boxes\"]\n",
    "                    boxes[:, 2] += boxes[:, 0]\n",
    "                    boxes[:, 3] += boxes[:, 1]\n",
    "                    target[\"boxes\"] = boxes\n",
    "\n",
    "                targets = filter_and_correct_boxes(targets)\n",
    "                targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "                outputs = model(images)\n",
    "\n",
    "                for i, output in enumerate(outputs):\n",
    "                    pred_boxes = output[\"boxes\"]\n",
    "                    true_boxes = targets[i][\"boxes\"]\n",
    "                    if pred_boxes.size(0) == 0 or true_boxes.size(0) == 0:\n",
    "                        continue\n",
    "                    iou = box_iou(pred_boxes, true_boxes)\n",
    "                    iou_list.append(iou.mean().item())\n",
    "\n",
    "                    pred_labels = output[\"labels\"]\n",
    "                    true_labels = targets[i][\"labels\"]\n",
    "\n",
    "                    # Ensure both tensors are the same size for comparison\n",
    "                    min_size = min(len(pred_labels), len(true_labels))\n",
    "                    correct_predictions += (\n",
    "                        (pred_labels[:min_size] == true_labels[:min_size]).sum().item()\n",
    "                    )\n",
    "                    total_predictions += min_size\n",
    "\n",
    "        mean_iou = sum(iou_list) / len(iou_list) if iou_list else 0\n",
    "        accuracy = correct_predictions / total_predictions if total_predictions else 0\n",
    "        print(f\"Mean IoU: {mean_iou:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "        return mean_iou, accuracy\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for i, (images, targets) in enumerate(data_loader):\n",
    "            images = [image.to(device) for image in images]\n",
    "            targets = [\n",
    "                {\n",
    "                    \"boxes\": torch.tensor(\n",
    "                        [obj[\"bbox\"] for obj in t], dtype=torch.float32\n",
    "                    ).to(device),\n",
    "                    \"labels\": torch.tensor(\n",
    "                        [obj[\"category_id\"] for obj in t], dtype=torch.int64\n",
    "                    ).to(device),\n",
    "                }\n",
    "                for t in targets\n",
    "            ]\n",
    "            for target in targets:\n",
    "                boxes = target[\"boxes\"]\n",
    "                boxes[:, 2] += boxes[:, 0]\n",
    "                boxes[:, 3] += boxes[:, 1]\n",
    "                target[\"boxes\"] = boxes\n",
    "\n",
    "            targets = filter_and_correct_boxes(targets)\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            loss_dict = model(images, targets)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            losses.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if i % 10 == 0:\n",
    "                print(\n",
    "                    f\"Epoch [{epoch}/{num_epochs}], Step [{i}/{len(data_loader)}], Loss: {losses.item():.4f}\"\n",
    "                )\n",
    "\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        mean_iou, accuracy = evaluate_model(model, test_data_loader)\n",
    "        if mean_iou > best_mean_iou:\n",
    "            best_mean_iou = mean_iou\n",
    "            torch.save(model.state_dict(), os.path.join(model_dir, \"best_model.pth\"))\n",
    "            print(\"Best model saved\")\n",
    "\n",
    "    print(\"Training completed.\")\n",
    "    return model\n",
    "\n",
    "\n",
    "# %% ------------------------------\n",
    "# evaluate model - task\n",
    "# ---------------------------------\n",
    "@task(container_image=image,\n",
    "      enable_deck=True,\n",
    "      requests=Resources(cpu=\"2\", mem=\"8Gi\", gpu=\"1\"))\n",
    "def evaluate_model(model: torch.nn.Module, dataset_dir: FlyteDirectory) -> str:\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    dataset_dir.download()\n",
    "    local_dataset_dir = dataset_dir.path\n",
    "    data_loader = dataset_dataloader(root=local_dataset_dir, \n",
    "                                     annFile=\"train.json\", shuffle=False)\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    num_images = 9  # Number of images to display in the grid\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(15, 15))  # Create a 3x3 grid\n",
    "    axes = axes.flatten()  # Flatten the axes array for easier iteration\n",
    "\n",
    "    iou_list, accuracy_list = [], []\n",
    "    report = []  # To store the IoU and accuracy report for each image\n",
    "    global_image_index = 0  # Global image counter across batches\n",
    "    images_plotted = 0  # Counter for images plotted in the grid\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, targets) in enumerate(data_loader):\n",
    "            images = [image.to(device) for image in images]\n",
    "            targets = [\n",
    "                {\n",
    "                    \"boxes\": torch.tensor(\n",
    "                        [obj[\"bbox\"] for obj in t], dtype=torch.float32\n",
    "                    ).to(device),\n",
    "                    \"labels\": torch.tensor(\n",
    "                        [obj[\"category_id\"] for obj in t], dtype=torch.int64\n",
    "                    ).to(device),\n",
    "                }\n",
    "                for t in targets\n",
    "            ]\n",
    "            for target in targets:\n",
    "                boxes = target[\"boxes\"]\n",
    "                boxes[:, 2] += boxes[:, 0]  # Convert width to x_max\n",
    "                boxes[:, 3] += boxes[:, 1]  # Convert height to y_max\n",
    "                target[\"boxes\"] = boxes\n",
    "\n",
    "            outputs = model(images)\n",
    "\n",
    "            for i, output in enumerate(outputs):\n",
    "                pred_boxes = output[\"boxes\"]\n",
    "                true_boxes = targets[i][\"boxes\"]\n",
    "\n",
    "                # Get the global image index\n",
    "                image_index = global_image_index + i\n",
    "\n",
    "                if pred_boxes.size(0) == 0 or true_boxes.size(0) == 0:\n",
    "                    report.append(f\"Image {image_index}: No valid predictions or ground truths\")\n",
    "                    continue\n",
    "\n",
    "                # Calculate IoU\n",
    "                iou = box_iou(pred_boxes, true_boxes)\n",
    "                mean_iou = iou.mean().item()\n",
    "                iou_list.append(mean_iou)\n",
    "\n",
    "                # Calculate accuracy\n",
    "                pred_labels = output[\"labels\"]\n",
    "                true_labels = targets[i][\"labels\"]\n",
    "                min_size = min(len(pred_labels), len(true_labels))\n",
    "                correct_predictions = (pred_labels[:min_size] == true_labels[:min_size]).sum().item()\n",
    "                accuracy = correct_predictions / min_size if min_size else 0\n",
    "                accuracy_list.append(accuracy)\n",
    "\n",
    "                # Append report for this image\n",
    "                report.append(f\"Image {image_index}: IoU = {mean_iou:.4f}, Accuracy = {accuracy:.4f}\")\n",
    "\n",
    "                # Plot images (limit to num_images)\n",
    "                if images_plotted < num_images:\n",
    "                    img = images[i].cpu().permute(1, 2, 0)  # Convert image to HWC format for plotting\n",
    "                    ax = axes[images_plotted]  # Access the correct subplot\n",
    "\n",
    "                    ax.imshow(img)\n",
    "                    for j in range(len(output['boxes'])):\n",
    "                        bbox = output['boxes'][j].cpu().numpy()\n",
    "                        score = output['scores'][j].cpu().item()\n",
    "                        label = output['labels'][j].cpu().item()\n",
    "\n",
    "                        if score > 0.6:  # Only display predictions with confidence score above 0.6\n",
    "                            rect = patches.Rectangle((bbox[0], bbox[1]), bbox[2] - bbox[0], bbox[3] - bbox[1],\n",
    "                                                     linewidth=2, edgecolor='r', facecolor='none')\n",
    "                            ax.add_patch(rect)\n",
    "                            ax.text(bbox[0], bbox[1], f\"{label}: {score:.2f}\", color=\"white\", fontsize=8,\n",
    "                                    bbox=dict(facecolor=\"red\", alpha=0.5))\n",
    "                    ax.axis('off')  # Hide axes\n",
    "                    images_plotted += 1\n",
    "\n",
    "            # Update global image index after processing the batch\n",
    "            global_image_index += len(images)\n",
    "\n",
    "            if images_plotted >= num_images:  # Break once we've plotted 9 images\n",
    "                break\n",
    "\n",
    "    # Compute overall metrics\n",
    "    overall_iou = sum(iou_list) / len(iou_list) if iou_list else 0\n",
    "    overall_accuracy = sum(accuracy_list) / len(accuracy_list) if accuracy_list else 0\n",
    "\n",
    "\n",
    "\n",
    "    # Save the image grid\n",
    "    pred_boxes_imgs = \"prediction_grid.png\"\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(pred_boxes_imgs)\n",
    "    plt.close()\n",
    "\n",
    "    train_image_base64 = image_to_base64(pred_boxes_imgs)\n",
    "\n",
    "    # Prepare the report as text\n",
    "    report_text = \"\\n\".join(report)\n",
    "    overall_report = dedent(f\"\"\"\n",
    "    Overall Metrics:\n",
    "    ----------------\n",
    "    Mean IoU: {overall_iou:.4f}\n",
    "    Mean Accuracy: {overall_accuracy:.4f}\n",
    "\n",
    "    Per-Image Metrics:\n",
    "    ------------------\n",
    "    {report_text}\n",
    "    \"\"\")\n",
    "\n",
    "    # Display the report in FlyteDeck\n",
    "    ctx = current_context()\n",
    "    deck = Deck(\"Evaluation Results\")\n",
    "    html_report = dedent(f\"\"\"\n",
    "    <div style=\"font-family: Arial, sans-serif; line-height: 1.6;\">\n",
    "       <h2 style=\"color: #2C3E50;\">Predicted Bounding Boxes</h2>\n",
    "        <img src=\"data:image/png;base64,{train_image_base64}\" width=\"600\">\n",
    "    </div>               \n",
    "    <div style=\"font-family: Arial, sans-serif; line-height: 1.6;\">\n",
    "        <h2 style=\"color: #2C3E50;\">Evaluation Report</h2>\n",
    "        <pre>{overall_report}</pre>\n",
    "    </div>\n",
    "\n",
    "    \"\"\")\n",
    "\n",
    "    # Append HTML content to the deck\n",
    "    deck.append(html_report)\n",
    "    ctx.decks.insert(0, deck)\n",
    "\n",
    "    return overall_report\n",
    "\n",
    "# %%\n",
    "@workflow\n",
    "def object_detection_workflow() -> torch.nn.Module:\n",
    "    dataset_dir = download_hf_dataset(repo_id=\"sagecodes/union_flyte_swag_object_detection\")\n",
    "    model = download_model()\n",
    "    trained_model = train_model(model=model, dataset_dir=dataset_dir, num_epochs=3)\n",
    "    evaluate_model(model=trained_model, dataset_dir=dataset_dir)\n",
    "    return model\n",
    "\n",
    "# union run --remote workflow2.py object_detection_workflow\n",
    "# union run workflow2.py object_detection_workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# union run --remote workflow2.py object_detection_workflow\n",
    "# union run workflow2.py object_detection_workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "\u001b[36mRunning Execution on Remote.\u001b[0m\n",
      "\u001b[34mImage cr.union.ai/flytekit:ETQmGXAp8aLajS6MMq0L0w found. Skip building.\u001b[0m\n",
      "\u001b[31mRequest rejected by the API, due to Invalid input.\u001b[0m\n",
      "\u001b[31m\u001b[1mRPC Failed, with Status: StatusCode.INVALID_ARGUMENT\u001b[0m\n",
      "\u001b[35m\u001b[1m\tDetails: failed to compile workflow for [resource_type:WORKFLOW project:\"default\" domain:\"development\" name:\"workflow2.object_detection_workflow\" version:\"9RJG8xRl3ZvFJler-XXQtg\" org:\"sagecodes\"] with err failed to compile workflow with err Collected Errors: 2\n",
      "\tError 0: Code: MismatchingTypes, Node Id: n3, Description: Variable [o0] (type [blob:{}]) doesn't match expected type [blob:{format:\"PyTorchModule\"}].\n",
      "\tError 1: Code: ParameterNotBound, Node Id: n3, Description: Parameter not bound [model].\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!union run --remote workflow.py object_detection_workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize remote context\n",
    "from union.remote import UnionRemote\n",
    "remote = UnionRemote()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cvpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
