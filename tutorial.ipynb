{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning RCNN Object Detection Pipeline\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/unionai-oss/object-detection-computer-vision-pipeline/blob/main/tutorial.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "GitHub repository: https://github.com/unionai-oss/object-detection-computer-vision-pipeline\n",
    "\n",
    "This notebook is a pipeline for fine-tuning a fast rcnn model on a custom dataset with PyTorch\n",
    "\n",
    "## Project Setup:\n",
    "\n",
    "Sign up for Union while libraries are installing below:\n",
    "\n",
    "- Union sign up: https://signup.union.ai/\n",
    "- Union Dashboard: https://serverless.union.ai/\n",
    "\n",
    "Install python libraries by running the code cell below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install required libraries\n",
    "!pip install python-dotenv union==0.1.64 torch torchvision matplotlib pycocotools datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "!pip install python-dotenv union==0.1.85 torch torchvision matplotlib pycocotools datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auth Union account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!union create login --serverless --auth device-flow "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Simple Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile simple_wf.py\n",
    "\n",
    "# Import libraries and modules\n",
    "# task\n",
    "from flytekit import task, workflow\n",
    "\n",
    "@task\n",
    "def hello_world(name: str) -> str:\n",
    "    return f\"Hello {name}\"\n",
    "\n",
    "# workflow\n",
    "@workflow\n",
    "def main(name: str) -> str:\n",
    "    return hello_world(name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run locally\n",
    "!union run simple_wf.py main --name Flyte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run on Union\n",
    "!union run --remote simple_wf.py main --name Flyte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!union run simple_wf.py main --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!union run --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine tune Fast RCNN Model for Object Detection\n",
    "- Download the dataset\n",
    "- Download the model\n",
    "- Train the model\n",
    "- Evaluate the model\n",
    "\n",
    "Use the model on new images or video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile workflow.py\n",
    "\"\"\"\n",
    "Object Detection Workflow with Flyte and PyTorch using the Faster R-CNN model\n",
    "\n",
    "note: This Flyte workflow can be broken out into modular tasks for better organization and reusability!\n",
    "\"\"\"\n",
    "# %%\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from flytekit import task, workflow, ImageSpec, Resources, current_context, Deck, Secret\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CocoDetection\n",
    "from torchvision.models.detection.faster_rcnn import \\\n",
    "    FasterRCNN_ResNet50_FPN_Weights\n",
    "from torchvision.models.detection import fasterrcnn_mobilenet_v3_large_fpn\n",
    "from torchvision.models.detection.faster_rcnn import FasterRCNN_MobileNet_V3_Large_FPN_Weights, FasterRCNN_MobileNet_V3_Large_320_FPN_Weights\n",
    "from torchvision.models.detection.faster_rcnn import fasterrcnn_mobilenet_v3_large_fpn\n",
    "from torchvision.models.detection.faster_rcnn import fasterrcnn_mobilenet_v3_large_320_fpn\n",
    "from torchvision.ops import box_iou\n",
    "from torchvision.transforms import transforms as T\n",
    "from flytekit.types.directory import FlyteDirectory\n",
    "from flytekit.types.file import FlyteFile\n",
    "import base64\n",
    "from textwrap import dedent\n",
    "from pathlib import Path\n",
    "\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from io import BytesIO\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Check and set the available device for local development\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "image = ImageSpec(\n",
    "    packages=[\n",
    "        \"union==0.1.85\",\n",
    "        \"flytekit==1.13.8\",\n",
    "        \"torch\",\n",
    "        \"torchvision\",\n",
    "        \"matplotlib\",\n",
    "        \"pycocotools\",\n",
    "        \"datasets\",\n",
    "        \"huggingface_hub\",\n",
    "        \"python-dotenv\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "# %% ------------------------------\n",
    "# helper functions\n",
    "# --------------------------------\n",
    "\n",
    "# Convert images to base64 and embed in HTML\n",
    "def image_to_base64(image_path):\n",
    "    with open(image_path, \"rb\") as img_file:\n",
    "        return base64.b64encode(img_file.read()).decode(\"utf-8\")\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "def dataset_dataloader(\n",
    "                    root: str,\n",
    "                    annFile: str,\n",
    "                    batch_size=2,\n",
    "                    shuffle=True,\n",
    "                    num_workers=0,\n",
    "                ) -> DataLoader:\n",
    "    # Define the transformations for the images\n",
    "    transform = T.Compose([T.ToTensor()])\n",
    "\n",
    "    annFile_path = os.path.join(str(root), annFile)\n",
    "    # Load the dataset\n",
    "    dataset = CocoDetection(root=root, annFile=annFile_path, transform=transform)\n",
    "    data_loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=num_workers,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "\n",
    "    return data_loader\n",
    "\n",
    "# Function to draw bounding boxes with confidence threshold\n",
    "def draw_boxes(image, boxes, labels, scores, labels_map, threshold=0.75):\n",
    "    # Load font for drawing\n",
    "    font_url = 'https://github.com/google/fonts/raw/main/apache/robotomono/RobotoMono%5Bwght%5D.ttf'\n",
    "    response = requests.get(font_url)\n",
    "    font = ImageFont.truetype(BytesIO(response.content), size=20)\n",
    "    draw = ImageDraw.Draw(image, 'RGBA')\n",
    "\n",
    "    colors = {\n",
    "        1: (255, 255, 100, 200),  # Example: Class 1 color\n",
    "        2: (128, 0, 128, 200),    # Example: Class 2 color\n",
    "    }\n",
    "    colors_fill = {\n",
    "        1: (255, 255, 0, 100),    # Example: Class 1 fill color\n",
    "        2: (128, 0, 128, 100),    # Example: Class 2 fill color\n",
    "    }\n",
    "\n",
    "    for box, label, score in zip(boxes, labels, scores):\n",
    "        if score < threshold:\n",
    "            continue  # Skip predictions below the confidence threshold\n",
    "        color = colors.get(label, (0, 255, 0, 200))\n",
    "        fill_color = colors_fill.get(label, (0, 255, 0, 100))\n",
    "        draw.rectangle([(box[0], box[1]), (box[2], box[3])], outline=color, width=3)\n",
    "        draw.rectangle([(box[0], box[1]), (box[2], box[3])], fill=fill_color)\n",
    "        label_text = f\"{labels_map.get(label, 'Unknown')}: {score:.2f}\"\n",
    "        text_size = font.getsize(label_text)\n",
    "        draw.rectangle(\n",
    "            [(box[0], box[1] - text_size[1]), (box[0] + text_size[0], box[1])],\n",
    "            fill=color,\n",
    "        )\n",
    "        draw.text(\n",
    "            (box[0], box[1] - text_size[1]),\n",
    "            label_text,\n",
    "            fill=\"white\",\n",
    "            font=font,\n",
    "        )\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "# %% ------------------------------\n",
    "# Download dataset - task\n",
    "# --------------------------------\n",
    "@task(container_image=image,\n",
    "      enable_deck=True,\n",
    "      cache=True,\n",
    "      cache_version=\"1.2\",\n",
    "      requests=Resources(cpu=\"2\", mem=\"2Gi\")) \n",
    "\n",
    "def download_hf_dataset(repo_id: str = 'sagecodes/union_swag_coco',\n",
    "                        local_dir: str = \"dataset\",\n",
    "                        sub_folder: str = \"swag\") -> FlyteDirectory:\n",
    "    \n",
    "    from huggingface_hub import snapshot_download\n",
    "\n",
    "    if local_dir:\n",
    "        dataset_dir = os.path.join(local_dir)\n",
    "        os.makedirs(dataset_dir, exist_ok=True)\n",
    "\n",
    "    # Download the dataset repository\n",
    "    repo_path = snapshot_download(repo_id=repo_id, \n",
    "                                  repo_type=\"dataset\",\n",
    "                                  local_dir=local_dir)\n",
    "    if sub_folder:\n",
    "        repo_path = os.path.join(repo_path, sub_folder)\n",
    "        # use sub_folder to return a specific folder from the dataset\n",
    "\n",
    "    print(f\"Dataset downloaded to {repo_path}\")\n",
    "\n",
    "    print(f\"Files in dataset directory: {os.listdir(repo_path)}\")\n",
    "\n",
    "    return FlyteDirectory(repo_path)\n",
    "\n",
    "# %% ------------------------------\n",
    "# visualize data - task\n",
    "# --------------------------------\n",
    "@task(container_image=image,\n",
    "      enable_deck=True,\n",
    "      requests=Resources(cpu=\"2\", mem=\"4Gi\"))\n",
    "def verify_data_and_annotations(dataset_dir: FlyteDirectory) -> FlyteFile:\n",
    "    \n",
    "    import matplotlib.patches as patches\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # Download the dataset locally from the FlyteDirectory\n",
    "    dataset_dir.download()\n",
    "    local_dataset_dir = dataset_dir.path\n",
    "    \n",
    "    # Load the dataset\n",
    "    data_loader = dataset_dataloader(root=local_dataset_dir, annFile=\"train.json\", shuffle=True)\n",
    "    \n",
    "    # Number of images to display\n",
    "    num_images = 9\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(15, 15))  # Create a 3x3 grid\n",
    "    axes = axes.flatten()  # Flatten the axes array for easier iteration\n",
    "    \n",
    "    images_plotted = 0  # Counter for images plotted\n",
    "\n",
    "    # Plot images along with annotations\n",
    "    for batch_idx, (images, targets) in enumerate(data_loader):\n",
    "        for i, image in enumerate(images):\n",
    "            if images_plotted >= num_images:\n",
    "                break  # Limit to 9 images\n",
    "            \n",
    "            # Plot the image\n",
    "            img = image.cpu().permute(1, 2, 0)  # Convert image to HWC format for plotting\n",
    "            ax = axes[images_plotted]  # Access the correct subplot\n",
    "            ax.imshow(img)\n",
    "\n",
    "            # Iterate over the list of annotations (objects) for the current image\n",
    "            for annotation in targets[i]:\n",
    "                # Extract the bounding box\n",
    "                bbox = annotation['bbox']  # This is in [x_min, y_min, width, height] format\n",
    "                \n",
    "                # Convert [x_min, y_min, width, height] to [x_min, y_min, x_max, y_max]\n",
    "                x_min, y_min, width, height = bbox\n",
    "                x_max = x_min + width\n",
    "                y_max = y_min + height\n",
    "\n",
    "                # Draw the bounding box\n",
    "                rect = patches.Rectangle((x_min, y_min), width, height,\n",
    "                                         linewidth=2, edgecolor='r', facecolor='none')\n",
    "                ax.add_patch(rect)\n",
    "\n",
    "            # Increment image counter\n",
    "            images_plotted += 1\n",
    "\n",
    "        if images_plotted >= num_images:\n",
    "            break  # Stop if we've plotted the desired number of images\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the grid of images and annotations\n",
    "    output_img = \"data_verification_grid.png\"\n",
    "    plt.savefig(output_img)\n",
    "    plt.close()\n",
    "\n",
    "    # Convert the image to base64 for display in FlyteDeck\n",
    "    verification_image_base64 = image_to_base64(output_img)\n",
    "\n",
    "    # Display the results in FlyteDeck\n",
    "    ctx = current_context()\n",
    "    deck = Deck(\"Data Verification\")\n",
    "    html_report = dedent(f\"\"\"\n",
    "    <div style=\"font-family: Arial, sans-serif; line-height: 1.6;\">\n",
    "       <h2 style=\"color: #2C3E50;\">Data Verification: Images and Annotations</h2>\n",
    "        <img src=\"data:image/png;base64,{verification_image_base64}\" width=\"600\">\n",
    "    </div>\n",
    "    \"\"\")\n",
    "\n",
    "    # Append the HTML content to the deck\n",
    "    deck.append(html_report)\n",
    "    ctx.decks.insert(0, deck)\n",
    "\n",
    "    # Return the image file for further use in the workflow\n",
    "    return FlyteFile(output_img)\n",
    "\n",
    "\n",
    "\n",
    "# %% ------------------------------\n",
    "# donwload model - task\n",
    "# --------------------------------\n",
    "@task(container_image=image,\n",
    "    cache=True,\n",
    "    cache_version=\"1.5\",\n",
    "    requests=Resources(cpu=\"2\", mem=\"2Gi\"))\n",
    "def download_model() -> torch.nn.Module:\n",
    "    # Load a pre-trained model\n",
    "    # model = torchvision.models.detection.fasterrcnn_resnet50_fpn(\n",
    "    #     weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1, weights_only=True\n",
    "    # )\n",
    "\n",
    "    # model = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_fpn(\n",
    "    #     weights=fasterrcnn_mobilenet_v3_large_fpn, weights_only=True\n",
    "    # )\n",
    "\n",
    "    model = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_320_fpn(\n",
    "        weights=FasterRCNN_MobileNet_V3_Large_320_FPN_Weights, weights_only=True\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "# %% ------------------------------\n",
    "# train model - task\n",
    "# --------------------------------\n",
    "@task(container_image=image,\n",
    "    requests=Resources(cpu=\"2\", mem=\"8Gi\", gpu=\"1\"))\n",
    "def train_model(model: torch.nn.Module, dataset_dir: FlyteDirectory, num_epochs :int=10) -> torch.nn.Module:\n",
    "\n",
    "    # TODO: make from dict\n",
    "    num_classes = 3  # number of classes + background (TODO: add one for the background class automatically)\n",
    "    num_epochs = num_epochs\n",
    "    best_mean_iou = 0\n",
    "    model_dir = \"models\"\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    dataset_dir.download()\n",
    "\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "    local_dataset_dir = dataset_dir.path  # Use the local path for FlyteDirectory\n",
    "\n",
    "    data_loader = dataset_dataloader(root=local_dataset_dir, annFile=\"train.json\")\n",
    "    test_data_loader = dataset_dataloader(root=local_dataset_dir, annFile=\"train.json\")\n",
    "\n",
    "    # Modify the model to add a new classification head based on the number of classes\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = (\n",
    "        torchvision.models.detection.faster_rcnn.FastRCNNPredictor(\n",
    "            in_features, num_classes\n",
    "        )\n",
    "    )\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    # Define optimizer and learning rate\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = optim.SGD(params, lr=0.001, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "    # Learning rate scheduler\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "\n",
    "    # Function to filter out and correct invalid boxes\n",
    "    def filter_and_correct_boxes(targets):\n",
    "        filtered_targets = []\n",
    "        for target in targets:\n",
    "            boxes = target[\"boxes\"]\n",
    "            labels = target[\"labels\"]\n",
    "            valid_indices = []\n",
    "            for i, box in enumerate(boxes):\n",
    "                if box[2] > box[0] and box[3] > box[1]:\n",
    "                    valid_indices.append(i)\n",
    "                else:\n",
    "                    print(f\"Invalid box found and removed: {box}\")\n",
    "            filtered_boxes = boxes[valid_indices]\n",
    "            filtered_labels = labels[valid_indices]\n",
    "            filtered_targets.append(\n",
    "                {\"boxes\": filtered_boxes, \"labels\": filtered_labels}\n",
    "            )\n",
    "        return filtered_targets\n",
    "\n",
    "    # Function to evaluate the model\n",
    "    def evaluate_model(model, data_loader):\n",
    "        model.eval()\n",
    "        iou_list, loss_list = [], []\n",
    "        correct_predictions, total_predictions = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for images, targets in data_loader:\n",
    "                images = [image.to(device) for image in images]\n",
    "                targets = [\n",
    "                    {\n",
    "                        \"boxes\": torch.tensor(\n",
    "                            [obj[\"bbox\"] for obj in t], dtype=torch.float32\n",
    "                        ).to(device),\n",
    "                        \"labels\": torch.tensor(\n",
    "                            [obj[\"category_id\"] for obj in t], dtype=torch.int64\n",
    "                        ).to(device),\n",
    "                    }\n",
    "                    for t in targets\n",
    "                ]\n",
    "                for target in targets:\n",
    "                    boxes = target[\"boxes\"]\n",
    "                    boxes[:, 2] += boxes[:, 0]\n",
    "                    boxes[:, 3] += boxes[:, 1]\n",
    "                    target[\"boxes\"] = boxes\n",
    "\n",
    "                targets = filter_and_correct_boxes(targets)\n",
    "                targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "                outputs = model(images)\n",
    "\n",
    "                for i, output in enumerate(outputs):\n",
    "                    pred_boxes = output[\"boxes\"]\n",
    "                    true_boxes = targets[i][\"boxes\"]\n",
    "                    if pred_boxes.size(0) == 0 or true_boxes.size(0) == 0:\n",
    "                        continue\n",
    "                    iou = box_iou(pred_boxes, true_boxes)\n",
    "                    iou_list.append(iou.mean().item())\n",
    "\n",
    "                    pred_labels = output[\"labels\"]\n",
    "                    true_labels = targets[i][\"labels\"]\n",
    "\n",
    "                    # Ensure both tensors are the same size for comparison\n",
    "                    min_size = min(len(pred_labels), len(true_labels))\n",
    "                    correct_predictions += (\n",
    "                        (pred_labels[:min_size] == true_labels[:min_size]).sum().item()\n",
    "                    )\n",
    "                    total_predictions += min_size\n",
    "\n",
    "        mean_iou = sum(iou_list) / len(iou_list) if iou_list else 0\n",
    "        accuracy = correct_predictions / total_predictions if total_predictions else 0\n",
    "        print(f\"Mean IoU: {mean_iou:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "        return mean_iou, accuracy\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for i, (images, targets) in enumerate(data_loader):\n",
    "            images = [image.to(device) for image in images]\n",
    "            targets = [\n",
    "                {\n",
    "                    \"boxes\": torch.tensor(\n",
    "                        [obj[\"bbox\"] for obj in t], dtype=torch.float32\n",
    "                    ).to(device),\n",
    "                    \"labels\": torch.tensor(\n",
    "                        [obj[\"category_id\"] for obj in t], dtype=torch.int64\n",
    "                    ).to(device),\n",
    "                }\n",
    "                for t in targets\n",
    "            ]\n",
    "            for target in targets:\n",
    "                boxes = target[\"boxes\"]\n",
    "                boxes[:, 2] += boxes[:, 0]\n",
    "                boxes[:, 3] += boxes[:, 1]\n",
    "                target[\"boxes\"] = boxes\n",
    "\n",
    "            targets = filter_and_correct_boxes(targets)\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            loss_dict = model(images, targets)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            losses.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if i % 10 == 0:\n",
    "                print(\n",
    "                    f\"Epoch [{epoch}/{num_epochs}], Step [{i}/{len(data_loader)}], Loss: {losses.item():.4f}\"\n",
    "                )\n",
    "\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        mean_iou, accuracy = evaluate_model(model, test_data_loader)\n",
    "        if mean_iou > best_mean_iou:\n",
    "            best_mean_iou = mean_iou\n",
    "            torch.save(model.state_dict(), os.path.join(model_dir, \"best_model.pth\"))\n",
    "            print(\"Best model saved\")\n",
    "\n",
    "    print(\"Training completed.\")\n",
    "    return model\n",
    "\n",
    "\n",
    "# %% ------------------------------\n",
    "# evaluate model - task\n",
    "# ---------------------------------\n",
    "@task(container_image=image,\n",
    "      enable_deck=True,\n",
    "      requests=Resources(cpu=\"2\", mem=\"8Gi\", gpu=\"1\"))\n",
    "def evaluate_model(model: torch.nn.Module, dataset_dir: FlyteDirectory, threshold: float = 0.75) -> str:\n",
    "    \n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    dataset_dir.download()\n",
    "    local_dataset_dir = dataset_dir.path\n",
    "    data_loader = dataset_dataloader(root=local_dataset_dir, \n",
    "                                     annFile=\"train.json\", shuffle=False)\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    num_images = 9  # Number of images to display in the grid\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(15, 15))  # Create a 3x3 grid\n",
    "    axes = axes.flatten()  # Flatten the axes array for easier iteration\n",
    "\n",
    "    iou_list, accuracy_list = [], []\n",
    "    report = []  # To store the IoU and accuracy report for each image\n",
    "    global_image_index = 0  # Global image counter across batches\n",
    "    images_plotted = 0  # Counter for images plotted in the grid\n",
    "\n",
    "    correct_predictions, total_predictions = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, targets) in enumerate(data_loader):\n",
    "            images = [image.to(device) for image in images]\n",
    "            targets = [\n",
    "                {\n",
    "                    \"boxes\": torch.tensor(\n",
    "                        [obj[\"bbox\"] for obj in t], dtype=torch.float32\n",
    "                    ).to(device),\n",
    "                    \"labels\": torch.tensor(\n",
    "                        [obj[\"category_id\"] for obj in t], dtype=torch.int64\n",
    "                    ).to(device),\n",
    "                }\n",
    "                for t in targets\n",
    "            ]\n",
    "            for target in targets:\n",
    "                boxes = target[\"boxes\"]\n",
    "                boxes[:, 2] += boxes[:, 0]  # Convert width to x_max\n",
    "                boxes[:, 3] += boxes[:, 1]  # Convert height to y_max\n",
    "                target[\"boxes\"] = boxes\n",
    "\n",
    "            outputs = model(images)\n",
    "\n",
    "            for i, output in enumerate(outputs):\n",
    "                pred_boxes = output[\"boxes\"]\n",
    "                pred_scores = output[\"scores\"]\n",
    "                pred_labels = output[\"labels\"]\n",
    "                true_boxes = targets[i][\"boxes\"]\n",
    "                true_labels = targets[i][\"labels\"]\n",
    "\n",
    "                # Filter predictions by confidence threshold\n",
    "                high_conf_indices = pred_scores > threshold\n",
    "                pred_boxes = pred_boxes[high_conf_indices]\n",
    "                pred_labels = pred_labels[high_conf_indices]\n",
    "\n",
    "                # Get the global image index\n",
    "                image_index = global_image_index + i\n",
    "\n",
    "                if pred_boxes.size(0) == 0 or true_boxes.size(0) == 0:\n",
    "                    report.append(f\"Image {image_index}: No valid predictions or ground truths\")\n",
    "                    continue\n",
    "\n",
    "                # Calculate IoU and match predictions to ground truth based on IoU\n",
    "                iou = box_iou(pred_boxes, true_boxes)\n",
    "                max_iou_indices = iou.argmax(dim=1)  # Find the best matching true box for each predicted box\n",
    "\n",
    "                # Calculate accuracy based on matching boxes\n",
    "                matched_true_labels = true_labels[max_iou_indices]  # Match true labels with best IoU\n",
    "                correct_predictions += (pred_labels == matched_true_labels).sum().item()\n",
    "                total_predictions += len(pred_labels)\n",
    "\n",
    "                # Calculate mean IoU\n",
    "                mean_iou = iou.max(dim=1)[0].mean().item()  # Get the highest IoU for each predicted box\n",
    "                iou_list.append(mean_iou)\n",
    "\n",
    "                # Append report for this image\n",
    "                accuracy = correct_predictions / total_predictions if total_predictions else 0\n",
    "                report.append(f\"Image {image_index}: IoU = {mean_iou:.4f}, Accuracy = {accuracy:.4f}\")\n",
    "\n",
    "                # Plot images (limit to num_images)\n",
    "                if images_plotted < num_images:\n",
    "                    img = images[i].cpu().permute(1, 2, 0)  # Convert image to HWC format for plotting\n",
    "                    ax = axes[images_plotted]  # Access the correct subplot\n",
    "\n",
    "                    ax.imshow(img)\n",
    "                    for j in range(len(pred_boxes)):\n",
    "                        bbox = pred_boxes[j].cpu().numpy()\n",
    "                        score = pred_scores[high_conf_indices][j].cpu().item()\n",
    "                        label = pred_labels[j].cpu().item()\n",
    "\n",
    "                        if score > threshold:  # Only display predictions with confidence score above threshold\n",
    "                            rect = patches.Rectangle((bbox[0], bbox[1]), bbox[2] - bbox[0], bbox[3] - bbox[1],\n",
    "                                                     linewidth=2, edgecolor='r', facecolor='none')\n",
    "                            ax.add_patch(rect)\n",
    "                            ax.text(bbox[0], bbox[1], f\"{label}: {score:.2f}\", color=\"white\", fontsize=8,\n",
    "                                    bbox=dict(facecolor=\"red\", alpha=0.5))\n",
    "                    ax.axis('off')  # Hide axes\n",
    "                    images_plotted += 1\n",
    "\n",
    "            # Update global image index after processing the batch\n",
    "            global_image_index += len(images)\n",
    "\n",
    "            if images_plotted >= num_images:  # Break once we've plotted 9 images\n",
    "                break\n",
    "\n",
    "    # Compute overall metrics\n",
    "    overall_iou = sum(iou_list) / len(iou_list) if iou_list else 0\n",
    "    overall_accuracy = correct_predictions / total_predictions if total_predictions else 0\n",
    "\n",
    "    # Save the image grid\n",
    "    pred_boxes_imgs = \"prediction_grid.png\"\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(pred_boxes_imgs)\n",
    "    plt.close()\n",
    "\n",
    "    train_image_base64 = image_to_base64(pred_boxes_imgs)\n",
    "\n",
    "    # Prepare the report as text\n",
    "    report_text = \"\\n\".join(report)\n",
    "    overall_report = dedent(f\"\"\"\n",
    "    Overall Metrics:\n",
    "    ----------------\n",
    "    Mean IoU: {overall_iou:.4f}\n",
    "    Mean Accuracy: {overall_accuracy:.4f}\n",
    "\n",
    "    Per-Image Metrics:\n",
    "    ------------------\n",
    "    {report_text}\n",
    "    \"\"\")\n",
    "\n",
    "    # Display the report in FlyteDeck\n",
    "    ctx = current_context()\n",
    "    deck = Deck(\"Evaluation Results\")\n",
    "    html_report = dedent(f\"\"\"\n",
    "    <div style=\"font-family: Arial, sans-serif; line-height: 1.6;\">\n",
    "       <h2 style=\"color: #2C3E50;\">Predicted Bounding Boxes</h2>\n",
    "        <img src=\"data:image/png;base64,{train_image_base64}\" width=\"600\">\n",
    "    </div>               \n",
    "    <div style=\"font-family: Arial, sans-serif; line-height: 1.6;\">\n",
    "        <h2 style=\"color: #2C3E50;\">Evaluation Report</h2>\n",
    "        <pre>{overall_report}</pre>\n",
    "    </div>\n",
    "\n",
    "    \"\"\")\n",
    "\n",
    "    # Append HTML content to the deck\n",
    "    deck.append(html_report)\n",
    "    ctx.decks.insert(0, deck)\n",
    "\n",
    "    return overall_report\n",
    "\n",
    "\n",
    "# %% ------------------------------\n",
    "# upload model to hub - task\n",
    "# --------------------------------\n",
    "@task(\n",
    "    container_image=image,\n",
    "    requests=Resources(cpu=\"2\", mem=\"2Gi\"),\n",
    "    secret_requests=[Secret(group=None, key=\"hf_token\")],\n",
    ")\n",
    "def upload_model_to_hub(model: torch.nn.Module, repo_name: str = \"sagecodes/cv-object\") -> str:\n",
    "    from huggingface_hub import HfApi\n",
    "    # Get the Flyte context and define the model path\n",
    "    ctx = current_context()\n",
    "    model_path = \"best_model.pth\"  # Save the model locally as \"best_model.pth\"\n",
    "\n",
    "    # Save the model's state dictionary\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    # Set Hugging Face token from local environment or Flyte secrets\n",
    "    hf_token = os.getenv(\"HF_TOKEN\")\n",
    "    if hf_token is None:\n",
    "        # If HF_TOKEN is not found, attempt to get it from the Flyte secrets\n",
    "        hf_token = ctx.secrets.get(key=\"hf_token\")\n",
    "        print(\"Using Hugging Face token from Flyte secrets.\")\n",
    "    else:\n",
    "        print(\"Using Hugging Face token from environment variable.\")\n",
    "\n",
    "    # Create a new repository (if it doesn't exist) on Hugging Face Hub\n",
    "    api = HfApi()\n",
    "    api.create_repo(repo_name, token=hf_token, exist_ok=True)\n",
    "\n",
    "    # Upload the model to the Hugging Face repository\n",
    "    api.upload_file(\n",
    "        path_or_fileobj=model_path,      # Path to the local file\n",
    "        path_in_repo=\"pytorch_model.bin\", # Destination path in the repo\n",
    "        repo_id=repo_name,\n",
    "        commit_message=\"Upload Faster R-CNN model\",\n",
    "        token=hf_token\n",
    "    )\n",
    "\n",
    "    return f\"Model uploaded to Hugging Face Hub: https://huggingface.co/{repo_name}\"\n",
    "\n",
    "# %% ------------------------------\n",
    "# Object Detection Workflow\n",
    "# --------------------------------\n",
    "@workflow\n",
    "def object_detection_workflow() -> torch.nn.Module:\n",
    "    dataset_dir = download_hf_dataset(repo_id=\"sagecodes/union_flyte_swag_object_detection\")\n",
    "    verify_data_and_annotations(dataset_dir=dataset_dir)\n",
    "    model = download_model()\n",
    "    trained_model = train_model(model=model, dataset_dir=dataset_dir, num_epochs=2)\n",
    "    evaluate_model(model=trained_model, dataset_dir=dataset_dir)\n",
    "    upload_model_to_hub(model=trained_model, repo_name=\"sagecodes/cv-modelnet-object-320\")\n",
    "\n",
    "    return model\n",
    "\n",
    "# union run --remote workflow.py object_detection_workflow\n",
    "# union run workflow.py object_detection_workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# union run --remote workflow.py object_detection_workflow\n",
    "# union run workflow2.py object_detection_workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!union run --remote workflow.py object_detection_workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # initialize remote context\n",
    "# from union.remote import UnionRemote\n",
    "# remote = UnionRemote()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the Model to Run Locally\n",
    "\n",
    "You can use models hosted in Union to run batch predictions.\n",
    "\n",
    "But for to we'll pull the model from the Hugging Face hub and run it in the notebook.\n",
    "\n",
    "You can also use a similar method for deploying the model on an edge device, like a Raspberry Pi!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from huggingface_hub import hf_hub_download\n",
    "import os\n",
    "import requests\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from io import BytesIO\n",
    "from torchvision.transforms import functional as F\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set HF token if needed (for private repos, etc)\n",
    "\n",
    "# download model\n",
    "repo_id = \"sagecodes/cv-modelnet-object-320\" # replace with your repo_id to get YOUR model\n",
    "model_file = hf_hub_download(repo_id=repo_id, filename=\"pytorch_model.bin\")\n",
    "# save_path = \"pytorch_model.bin\"\n",
    "# Load the pre-trained Faster R-CNN model\n",
    "model = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_320_fpn(pretrained=False)\n",
    "\n",
    "# Modify the model to add a new classification head based on the number of classes\n",
    "num_classes = 3\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(\n",
    "    in_features, num_classes\n",
    ")\n",
    "\n",
    "# Load in fine-tuned model & set for inference\n",
    "model.load_state_dict(torch.load(model_file))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download images and video to run the model on\n",
    "\n",
    "In this case they're in the same dataset repository, but you can use your own images and videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_hf_dataset(repo_id: str = 'sagecodes/union_swag_coco',\n",
    "                        local_dir: str = \"dataset\",\n",
    "                        sub_folder: str = \"swag\"):\n",
    "    \n",
    "    from huggingface_hub import snapshot_download\n",
    "\n",
    "    if local_dir:\n",
    "        dataset_dir = os.path.join(local_dir)\n",
    "        os.makedirs(dataset_dir, exist_ok=True)\n",
    "\n",
    "    # Download the dataset repository\n",
    "    repo_path = snapshot_download(repo_id=repo_id, \n",
    "                                  repo_type=\"dataset\",\n",
    "                                  local_dir=local_dir)\n",
    "    if sub_folder:\n",
    "        repo_path = os.path.join(repo_path, sub_folder)\n",
    "        # use sub_folder to return a specific folder from the dataset\n",
    "\n",
    "    print(f\"Dataset downloaded to {repo_path}\")\n",
    "\n",
    "    print(f\"Files in dataset directory: {os.listdir(repo_path)}\")\n",
    "\n",
    "    return repo_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_hf_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### draw customized boxes on the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define labels map\n",
    "labels_map = {1: \"Union Sticker\", 2: \"Flyte Sticker\"}\n",
    "\n",
    "# Check and set the available device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "\n",
    "font_url = 'https://github.com/google/fonts/raw/main/apache/robotomono/RobotoMono%5Bwght%5D.ttf'\n",
    "response = requests.get(font_url)\n",
    "font = ImageFont.truetype(BytesIO(response.content), size=20)\n",
    "\n",
    "\n",
    "# Function to draw bounding boxes\n",
    "def draw_boxes(image, boxes, labels, scores, labels_map):\n",
    "    draw = ImageDraw.Draw(image, 'RGBA')\n",
    "    # font = ImageFont.truetype(urlopen(truetype_url), size=20)\n",
    "    # font = ImageFont.load_default() # default font in pil\n",
    "\n",
    "\n",
    "    colors = {\n",
    "        0: (255, 173, 10, 200),  # Class 0 color (e.g., blue)\n",
    "        1: (28, 140, 252, 200),  # Class 1 color (e.g., orange)\n",
    "    }\n",
    "    colors_fill = {\n",
    "        0: (255, 173, 10, 100),  # Class 0 fill color (e.g., bluea)\n",
    "        1: (28, 140, 252, 100),  # Class 1 fill color (e.g., orangea)\n",
    "    }\n",
    "\n",
    "    for box, label, score in zip(boxes, labels, scores):\n",
    "        if score > 0.6: # adjust threshold as needed\n",
    "          color = colors.get(label, (0, 255, 0, 200))\n",
    "          fill_color = colors_fill.get(label, (0, 255, 0, 100))\n",
    "          draw.rectangle([(box[0], box[1]), (box[2], box[3])], outline=color, width=3)\n",
    "          draw.rectangle([(box[0], box[1]), (box[2], box[3])], fill=fill_color)\n",
    "          label_text = f\"{labels_map[label]}: {score:.2f}\"\n",
    "          text_size = font.getbbox(label_text)\n",
    "          draw.rectangle([(box[0], box[1] - text_size[1]), (box[0] + text_size[0], box[1])], fill=color)\n",
    "          draw.text((box[0], box[1] - text_size[1]), label_text, fill=\"white\", font=font)\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "# Load a single test image\n",
    "image_path = '/content/dataset/swag/images/1bd5a6b5-20240916_133544.jpg'\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "image_tensor = F.to_tensor(image).unsqueeze(0).to(device)\n",
    "\n",
    "# Run inference\n",
    "with torch.no_grad():\n",
    "    outputs = model(image_tensor)\n",
    "\n",
    "# Get the boxes, labels, and scores\n",
    "boxes = outputs[0]['boxes'].cpu().numpy()\n",
    "labels = outputs[0]['labels'].cpu().numpy()\n",
    "scores = outputs[0]['scores'].cpu().numpy()\n",
    "\n",
    "# Define labels map\n",
    "labels_map = {0: \"Background\", 1: \"Union Sticker\", 2: \"Flyte Sticker\"}\n",
    "\n",
    "# Draw the boxes on the image\n",
    "image_with_boxes = draw_boxes(image, boxes, labels, scores, labels_map)\n",
    "\n",
    "# Display the image\n",
    "image_with_boxes.show()\n",
    "\n",
    "# Save the image\n",
    "image_with_boxes.save('output_image.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the model on a video file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define labels map\n",
    "labels_map = {1: \"Union Sticker\", 2: \"Flyte Sticker\"}\n",
    "\n",
    "# Check and set the available device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Load font from a URL\n",
    "font_url = 'https://github.com/google/fonts/raw/main/apache/robotomono/RobotoMono%5Bwght%5D.ttf'\n",
    "response = requests.get(font_url)\n",
    "font = ImageFont.truetype(BytesIO(response.content), size=20)\n",
    "\n",
    "\n",
    "# Video path and properties\n",
    "video_path = '/content/dataset/swag/videos/union_sticker_video.mp4'\n",
    "video = cv2.VideoCapture(video_path)\n",
    "width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "frames_per_second = video.get(cv2.CAP_PROP_FPS)\n",
    "num_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "# Initialize video writer\n",
    "video_writer = cv2.VideoWriter('out.mp4', cv2.VideoWriter_fourcc(*\"mp4v\"), fps=float(frames_per_second), frameSize=(width, height), isColor=True)\n",
    "\n",
    "# Function to process video frame by frame\n",
    "def run_inference_video(video, model, device, labels_map):\n",
    "    while True:\n",
    "        hasFrame, frame = video.read()\n",
    "        if not hasFrame:\n",
    "            break\n",
    "\n",
    "        # Convert frame to PIL image\n",
    "        image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "        image_tensor = F.to_tensor(image).unsqueeze(0).to(device)\n",
    "\n",
    "        # Run inference\n",
    "        with torch.no_grad():\n",
    "            outputs = model(image_tensor)\n",
    "\n",
    "        # Get the boxes, labels, and scores\n",
    "        boxes = outputs[0]['boxes'].cpu().numpy()\n",
    "        labels = outputs[0]['labels'].cpu().numpy()\n",
    "        scores = outputs[0]['scores'].cpu().numpy()\n",
    "\n",
    "        # Draw the boxes on the image\n",
    "        image_with_boxes = draw_boxes(image, boxes, labels, scores, labels_map)\n",
    "\n",
    "        # Convert back to OpenCV image format\n",
    "        result_frame = cv2.cvtColor(np.array(image_with_boxes), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        yield result_frame\n",
    "\n",
    "# Run inference and write video\n",
    "for frame in run_inference_video(video, model, device, labels_map):\n",
    "    video_writer.write(frame)\n",
    "\n",
    "# Release resources\n",
    "video.release()\n",
    "video_writer.release()\n",
    "# cv2.destroyAllWindows() # uncomment if running as a script with GUI build"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of running the model on a video feed (does not work in Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coming soon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keep learning & Building!\n",
    "\n",
    "- Label your own dataset\n",
    "- Try a more robust model back end\n",
    "- Try a different model type\n",
    "- Deploy the model on an edge device\n",
    "- Optiiize the model for more speed\n",
    "- Deploy the model on a Hugging Face Space\n",
    "\n",
    "### Resources:\n",
    "\n",
    "- Union.ai: https://union.ai/\n",
    "- Union Documentation: https://docs.union.ai/\n",
    "- Hugging Face: https://huggingface.co/\n",
    "- PyTorch Models: https://pytorch.org/vision/stable/models.html\n",
    "- LabelStudio: https://labelstud.io/\n",
    "\n",
    "Join the community: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cvpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
